#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ DocumentChunker –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –Ω–∞ —á–∞–Ω–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
"""

import json
from document_chunker import DocumentChunker


def demonstrate_chunking():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å —á–∞–Ω–∫–µ—Ä–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ"""
    
    print("=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ê–ó–ë–ò–í–ö–ò –î–û–ö–£–ú–ï–ù–¢–ê –ù–ê JSON –ß–ê–ù–ö–ò ===\n")
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —á–∞–Ω–∫–µ—Ä–∞
    chunker = DocumentChunker()
    
    # –ü—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É
    document_path = "../documents/Untitled.txt"
    
    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {document_path}")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    chunks = chunker.chunk_document(document_path)
    
    if not chunks:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏")
        return
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤\n")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º —Ä–∞–∑–¥–µ–ª–æ–≤
    section_stats = {}
    for chunk in chunks:
        section_type = chunk['meta']['section_type']
        level = chunk['meta']['level']
        key = f"{section_type}_level_{level}"
        section_stats[key] = section_stats.get(key, 0) + 1
    
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Ä–∞–∑–¥–µ–ª–æ–≤:")
    for key, count in sorted(section_stats.items()):
        print(f"   {key}: {count} —á–∞–Ω–∫–æ–≤")
    print()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤
    print("üìã –ü—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤:\n")
    
    # –ì–ª–∞–≤–Ω—ã–π —Ä–∞–∑–¥–µ–ª
    main_sections = [c for c in chunks if c['meta']['level'] == 0 and c['meta']['section_type'] == 'section']
    if main_sections:
        chunk = main_sections[0]
        print("üîπ –ì–ª–∞–≤–Ω—ã–π —Ä–∞–∑–¥–µ–ª:")
        print(f"   –ù–æ–º–µ—Ä: {chunk['meta']['section_number']}")
        print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {chunk['meta']['section_title']}")
        print(f"   –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {chunk['meta']['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –¢–µ–∫—Å—Ç: {chunk['text'][:100]}...")
        print()
    
    # –ü–æ–¥—Ä–∞–∑–¥–µ–ª —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π
    sub_sections = [c for c in chunks if c['meta']['level'] >= 2]
    if sub_sections:
        chunk = sub_sections[0]
        print("üî∏ –ü–æ–¥—Ä–∞–∑–¥–µ–ª —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π:")
        print(f"   –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {chunk['meta']['chunk_title']}")
        print(f"   –£—Ä–æ–≤–µ–Ω—å: {chunk['meta']['level']}")
        print(f"   –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã:")
        for parent in chunk['meta']['parent_sections']:
            print(f"     - {parent['number']} {parent['title'][:50]}...")
        print(f"   –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {chunk['meta']['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        print()
    
    # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    appendix_chunks = [c for c in chunks if c['meta']['section_type'] == 'appendix']
    if appendix_chunks:
        chunk = appendix_chunks[0]
        print("üìé –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
        print(f"   –ù–æ–º–µ—Ä: {chunk['meta']['section_number']}")
        print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {chunk['meta']['section_title']}")
        print(f"   –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {chunk['meta']['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        print()
    
    return chunks


def search_in_chunks():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ —á–∞–Ω–∫–∞—Ö"""
    
    print("=== –ü–û–ò–°–ö –í –ß–ê–ù–ö–ê–• ===\n")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞
    try:
        with open("../documents/chunks_json/all_chunks.json", 'r', encoding='utf-8') as f:
            chunks = json.load(f)
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª all_chunks.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ä–∞–∑–±–∏–≤–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
        return
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤\n")
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–∞
    search_terms = ["–ª–∏–∑–∏–Ω–≥–æ–≤—ã–π –ø–ª–∞—Ç–µ–∂", "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"]
    
    for term in search_terms:
        print(f"üîç –ü–æ–∏—Å–∫: '{term}'")
        found_chunks = []
        
        for chunk in chunks:
            if term.lower() in chunk['text'].lower():
                found_chunks.append(chunk)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(found_chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        for i, chunk in enumerate(found_chunks[:2]):
            print(f"   {i+1}. {chunk['meta']['section_number']} {chunk['meta']['section_title'][:50]}...")
            print(f"      –£—Ä–æ–≤–µ–Ω—å: {chunk['meta']['level']}, –†–∞–∑–º–µ—Ä: {chunk['meta']['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if len(found_chunks) > 2:
            print(f"      ... –∏ –µ—â–µ {len(found_chunks) - 2} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        print()


def analyze_chunk_structure():
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —á–∞–Ω–∫–æ–≤"""
    
    print("=== –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –ß–ê–ù–ö–û–í ===\n")
    
    try:
        with open("../documents/chunks_json/all_chunks.json", 'r', encoding='utf-8') as f:
            chunks = json.load(f)
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª all_chunks.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ä–∞–∑–±–∏–≤–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
        return
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ —É—Ä–æ–≤–Ω—è–º
    levels = {}
    for chunk in chunks:
        level = chunk['meta']['level']
        levels[level] = levels.get(level, 0) + 1
    
    print("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏:")
    for level in sorted(levels.keys()):
        print(f"   –£—Ä–æ–≤–µ–Ω—å {level}: {levels[level]} —á–∞–Ω–∫–æ–≤")
    print()
    
    # –°–∞–º—ã–µ –±–æ–ª—å—à–∏–µ –∏ –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
    chunks_by_size = sorted(chunks, key=lambda x: x['meta']['text_length'])
    
    print("üìè –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤:")
    print(f"   –°–∞–º—ã–π –º–∞–ª–µ–Ω—å–∫–∏–π: {chunks_by_size[0]['meta']['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –°–∞–º—ã–π –±–æ–ª—å—à–æ–π: {chunks_by_size[-1]['meta']['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {sum(c['meta']['text_length'] for c in chunks) // len(chunks)} —Å–∏–º–≤–æ–ª–æ–≤")
    print()
    
    # –°–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
    print("üìã –¢–æ–ø-5 —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤:")
    for i, chunk in enumerate(chunks_by_size[-5:], 1):
        print(f"   {i}. {chunk['meta']['section_number']} - {chunk['meta']['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"      {chunk['meta']['section_title'][:70]}...")
    print()


def export_chunk_metadata():
    """–≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ CSV"""
    
    print("=== –≠–ö–°–ü–û–†–¢ –ú–ï–¢–ê–î–ê–ù–ù–´–• ===\n")
    
    try:
        with open("../documents/chunks_json/all_chunks.json", 'r', encoding='utf-8') as f:
            chunks = json.load(f)
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª all_chunks.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ä–∞–∑–±–∏–≤–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
        return
    
    # –°–æ–∑–¥–∞–µ–º CSV —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    csv_content = "chunk_index,section_number,section_title,section_type,level,text_length,parent_count\n"
    
    for chunk in chunks:
        meta = chunk['meta']
        csv_content += f"{meta['chunk_index']},{meta['section_number']},\"{meta['section_title'][:50]}...\",{meta['section_type']},{meta['level']},{meta['text_length']},{len(meta['parent_sections'])}\n"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
    with open("../documents/chunks_json/chunks_metadata.csv", 'w', encoding='utf-8') as f:
        f.write(csv_content)
    
    print("‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ chunks_metadata.csv")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π
    chunks = demonstrate_chunking()
    
    if chunks:
        print("\n" + "="*60 + "\n")
        search_in_chunks()
        
        print("\n" + "="*60 + "\n")
        analyze_chunk_structure()
        
        print("\n" + "="*60 + "\n")
        export_chunk_metadata()
        
        print("\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!") 